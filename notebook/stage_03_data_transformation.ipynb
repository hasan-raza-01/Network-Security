{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.utils import read_yaml\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "CONFIG = read_yaml(\"config/config.yaml\")\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConstants:\n",
    "    ARITFACTS_ROOT_DIR_NAME = CONFIG.ARITFACTS_ROOT_DIR_NAME\n",
    "    DATA_ROOT_DIR_NAME = CONFIG.DATA.ROOT_DIR_NAME\n",
    "    TRANSFORMATION_ROOT_DIR_NAME = CONFIG.DATA.TRANSFORMATION.ROOT_DIR_NAME\n",
    "    PREPROCESSOR_NAME = CONFIG.DATA.TRANSFORMATION.PREPROCESSOR_NAME\n",
    "    TRAIN_FILE_NAME = CONFIG.DATA.TRANSFORMATION.TRAIN_FILE_NAME\n",
    "    TEST_FILE_NAME = CONFIG.DATA.TRANSFORMATION.TEST_FILE_NAME\n",
    "    TARGET_COLUMN_NAME = \"Result\"\n",
    "    PREPROCESSOR_PARAMS = dict(\n",
    "        missing_values = np.nan,\n",
    "        strategy = \"most_frequent\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ARITFACTS_ROOT_DIR_NAME:\", DataTransformationConstants.ARITFACTS_ROOT_DIR_NAME)\n",
    "print(\"DATA_ROOT_DIR_NAME:\", DataTransformationConstants.DATA_ROOT_DIR_NAME)\n",
    "print(\"TRANSFORMATION_ROOT_DIR_NAME:\", DataTransformationConstants.TRANSFORMATION_ROOT_DIR_NAME)\n",
    "print(\"PREPROCESSOR_NAME:\", DataTransformationConstants.PREPROCESSOR_NAME)\n",
    "print(\"TRAIN_FILE_NAME:\", DataTransformationConstants.TRAIN_FILE_NAME)\n",
    "print(\"TEST_FILE_NAME:\", DataTransformationConstants.TEST_FILE_NAME)\n",
    "print(\"TARGET_COLUMN_NAME:\", DataTransformationConstants.TARGET_COLUMN_NAME)\n",
    "print(\"TEST_FILE_NAME:\", DataTransformationConstants.PREPROCESSOR_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfigEntity:\n",
    "    ARITFACTS_ROOT_DIR_PATH: Path\n",
    "    DATA_ROOT_DIR_PATH: Path\n",
    "    TRANSFORMATION_ROOT_DIR_PATH: Path\n",
    "    PREPROCESSOR_PATH: Path\n",
    "    TRAIN_FILE_PATH: Path\n",
    "    TEST_FILE_PATH: Path\n",
    "    TARGET_COLUMN_NAME: str\n",
    "    PREPROCESSOR_PARAMS: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    ARITFACTS_ROOT_DIR_PATH =  Path(DataTransformationConstants.ARITFACTS_ROOT_DIR_NAME)\n",
    "    DATA_ROOT_DIR_PATH =  os.path.join(ARITFACTS_ROOT_DIR_PATH, DataTransformationConstants.DATA_ROOT_DIR_NAME)\n",
    "    TRANSFORMATION_ROOT_DIR_PATH =  os.path.join(DATA_ROOT_DIR_PATH, DataTransformationConstants.TRANSFORMATION_ROOT_DIR_NAME)\n",
    "    PREPROCESSOR_PATH =  os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.PREPROCESSOR_NAME)\n",
    "    TRAIN_FILE_PATH =  os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.TRAIN_FILE_NAME)\n",
    "    TEST_FILE_PATH =  os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.TEST_FILE_NAME)\n",
    "    TARGET_COLUMN_NAME =  DataTransformationConstants.TARGET_COLUMN_NAME\n",
    "    PREPROCESSOR_PARAMS =  DataTransformationConstants.PREPROCESSOR_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ARITFACTS_ROOT_DIR_PATH:\", DataTransformationConfig.ARITFACTS_ROOT_DIR_PATH)\n",
    "print(\"DATA_ROOT_DIR_PATH:\", DataTransformationConfig.DATA_ROOT_DIR_PATH)\n",
    "print(\"TRANSFORMATION_ROOT_DIR_PATH:\", DataTransformationConfig.TRANSFORMATION_ROOT_DIR_PATH)\n",
    "print(\"PREPROCESSOR_PATH:\", DataTransformationConfig.PREPROCESSOR_PATH)\n",
    "print(\"TRAIN_FILE_PATH:\", DataTransformationConfig.TRAIN_FILE_PATH)\n",
    "print(\"TEST_FILE_PATH:\", DataTransformationConfig.TEST_FILE_PATH)\n",
    "print(\"TARGET_COLUMN_NAME:\", DataTransformationConfig.TARGET_COLUMN_NAME)\n",
    "print(\"PREPROCESSOR_PARAMS:\", DataTransformationConfig.PREPROCESSOR_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.entity import DataValidationConfigEntity\n",
    "from package.exception import CustomException\n",
    "from package.utils import create_dirs, save_obj\n",
    "from dataclasses import dataclass\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationComponents:\n",
    "    data_validation_config: DataValidationConfigEntity\n",
    "    data_transformation_config: DataTransformationConfigEntity\n",
    "\n",
    "    def get_preprocessor(self)->Pipeline:\n",
    "        try:\n",
    "            params = self.data_transformation_config.PREPROCESSOR_PARAMS\n",
    "            imputer = SimpleImputer(**params)\n",
    "            preprocessor = Pipeline([(\"imputer\", imputer)])\n",
    "            return preprocessor\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def transform_data(self, train_data_path:Path, test_data_path:Path)->tuple[np.array]:\n",
    "        \"\"\"transform data with SimpleImputer only\n",
    "\n",
    "        Args:\n",
    "            train_data_path (Path): train file path\n",
    "            test_data_path (Path): test file path\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.array]: (train data, test data)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # get data\n",
    "            train_data = pd.read_csv(train_data_path)\n",
    "            test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "            # X, y for train data\n",
    "            target_column = self.data_transformation_config.TARGET_COLUMN_NAME\n",
    "            X_train  = train_data.drop(target_column, axis=1)\n",
    "            y_train = train_data[target_column].replace(-1, 0)\n",
    "\n",
    "            # X, y for test data\n",
    "            target_column = self.data_transformation_config.TARGET_COLUMN_NAME\n",
    "            X_test  = test_data.drop(target_column, axis=1)\n",
    "            y_test = test_data[target_column].replace(-1, 0)\n",
    "            \n",
    "            # get preprocessor object\n",
    "            preprocessor = self.get_preprocessor()\n",
    "\n",
    "            # save preprocessor\n",
    "            preprocessor_path = self.data_transformation_config.PREPROCESSOR_PATH\n",
    "            save_obj(preprocessor, preprocessor_path)\n",
    "\n",
    "            # transform data input features\n",
    "            transformed_X_train = preprocessor.fit_transform(X_train)\n",
    "            transformed_X_test = preprocessor.transform(X_test)\n",
    "\n",
    "            # concatination of input and output features\n",
    "            preprocessed_train_data = np.c_[transformed_X_train, np.array(y_train)]\n",
    "            preprocessed_test_data = np.c_[transformed_X_test, np.array(y_test)]\n",
    "\n",
    "            return (preprocessed_train_data, preprocessed_test_data)\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def intiate_transformation(self):\n",
    "        try:\n",
    "            # create required dir's\n",
    "            create_dirs(self.data_transformation_config.ARITFACTS_ROOT_DIR_PATH)\n",
    "            create_dirs(self.data_transformation_config.DATA_ROOT_DIR_PATH)\n",
    "            create_dirs(self.data_transformation_config.TRANSFORMATION_ROOT_DIR_PATH)\n",
    "\n",
    "            # get drift report\n",
    "            drift_report = read_yaml(self.data_validation_config.DRIFT_REPORT_FILE_PATH).result\n",
    "\n",
    "            # verify drift status\n",
    "            for _, status in drift_report.items():\n",
    "                if not status:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise CustomException(\"data drift status is True\", sys)\n",
    "                \n",
    "            # get train and test file path\n",
    "            train_data_path = self.data_validation_config.VALID_TRAIN_FILE_PATH\n",
    "            test_data_path = self.data_validation_config.VALID_TEST_FILE_PATH\n",
    "\n",
    "            # get transformed data\n",
    "            train_data, test_data = self.transform_data(train_data_path, test_data_path)\n",
    "\n",
    "            # save transformed train data\n",
    "            train_file_path = self.data_transformation_config.TRAIN_FILE_PATH\n",
    "            np.save(train_file_path, train_data)\n",
    "\n",
    "            # save transformed test data\n",
    "            test_file_path = self.data_transformation_config.TEST_FILE_PATH\n",
    "            np.save(test_file_path, test_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import  dataclass\n",
    "from package.configuration import DataValidationConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationPipeline:\n",
    "\n",
    "    def main(self)->None:\n",
    "        \"\"\"runs data ingestion full pipeline\n",
    "        \"\"\"\n",
    "        data_transformation = DataTransformationComponents(DataValidationConfig, DataTransformationConfig)\n",
    "        data_transformation.intiate_transformation()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "STAGE_NAME = \"Data Transformation\"\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(f\"\\n>>>>>>>>>>>>>>>>>>>>> {STAGE_NAME} initiated <<<<<<<<<<<<<<<<<<<<<\")\n",
    "    obj = DataTransformationPipeline()\n",
    "    obj.main()\n",
    "    print(f\"\\n>>>>>>>>>>>>>>>>>>>>> {STAGE_NAME} completed <<<<<<<<<<<<<<<<<<<<<\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(DataTransformationConfig.TRAIN_FILE_PATH)\n",
    "test_data = np.load(DataTransformationConfig.TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
